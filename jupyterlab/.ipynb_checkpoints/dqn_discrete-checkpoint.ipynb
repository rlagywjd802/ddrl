{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "supported-encounter",
   "metadata": {},
   "source": [
    "Jupyterlab Shortcuts\n",
    "- Shift + Enter : Run\n",
    "- Enter/Esc : Mode change(Edit/Command)\n",
    "- Fn + Up/Down : Move between cell \n",
    "- A / B : Insert Cell Above/Below\n",
    "- D, D : Delete selected cell"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "latest-nevada",
   "metadata": {},
   "source": [
    "Random\n",
    "- random.sample(population, k) : population sequence 에서 독립적으로 k length의 데이터를 sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "working-canberra",
   "metadata": {},
   "outputs": [],
   "source": [
    "Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "olympic-suspect",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "import numpy as np\n",
    "import random\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "incident-thirty",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 2\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "state = env.reset()\n",
    "\n",
    "# for discrete env\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.n\n",
    "\n",
    "print(state_dim, action_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lined-sample",
   "metadata": {},
   "source": [
    "Network\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "approximate-registration",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, action_dim)\n",
    "    \n",
    "    def forward(self, state):\n",
    "        out = F.relu(self.fc1(state))\n",
    "        out = F.relu(self.fc2(out))\n",
    "        out = self.fc3(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dedicated-november",
   "metadata": {},
   "outputs": [],
   "source": [
    "q = Net(state_dim, action_dim)\n",
    "qtarget = Net(state_dim, action_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "exterior-diana",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.3967, -0.3701,  0.4617, -0.1940])\n",
      "tensor([-0.3967, -0.3701,  0.4617, -0.1940])\n",
      "tensor([-0.3967, -0.3701,  0.4617, -0.1940])\n",
      "tensor([-0.3967, -0.3701,  0.4617, -0.1940])\n"
     ]
    }
   ],
   "source": [
    "# network의 weight값을 print\n",
    "# network의 weight값을 다른 network의 weight값으로 맞춰줌\n",
    "weight_name = 'fc1.weight'\n",
    "print(q.state_dict()[weight_name][1])\n",
    "print(qtarget.state_dict()[weight_name][1])\n",
    "qtarget.load_state_dict(q.state_dict())\n",
    "print(q.state_dict()[weight_name][1])\n",
    "print(qtarget.state_dict()[weight_name][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "another-plumbing",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, env, epsilon):\n",
    "        # if env is discrete,\n",
    "        state_dim = env.observation_space.shape[0]\n",
    "        action_dim = env.action_space.n\n",
    "        print(state_dim, action_dim)\n",
    "        self.epsilon = epsilon\n",
    "                \n",
    "        self.q = Net(state_dim, action_dim)\n",
    "        self.qtarget = Net(state_dim, action_dim)\n",
    "        \n",
    "        self.optimizer = optim.Adam(self.q.parameters())\n",
    "    \n",
    "    def update_qtarget_parameter(self):\n",
    "        self.qtarget.load_state_dict(self.q.state_dict())    \n",
    "    \n",
    "    def get_action(self, state):\n",
    "        # state : (ndarray) --> (tensor)\n",
    "        # action : (tensor) --> (int)\n",
    "        if self.epsilon < random.random():\n",
    "            selected_action = random.randrange(action_dim)\n",
    "        else:\n",
    "            qval = self.q.forward(torch.FloatTensor(state))\n",
    "            selected_action = qval.argmax().numpy()\n",
    "        return selected_action\n",
    "    \n",
    "    def learn(self, mini_batch):\n",
    "        # batch_size 만큼 크기의 loss의 합을 최소화\n",
    "        # zero_grad()를 매번 해줘야 하나?\n",
    "        # why l1 loss?\n",
    "        losses = []\n",
    "        for sample in mini_batch:\n",
    "            losses.append((torch.FloatTensor(sample['predict']) - self.q.forward(torch.FloatTensor(sample['state'])))^2)\n",
    "        loss = F.smooth_l1_loss()\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "    \n",
    "    def _compute_dqn_loss(self):\n",
    "        pass\n",
    "    \n",
    "    def _plot(self, scores):\n",
    "        plt.plot(scores)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "utility-petite",
   "metadata": {},
   "source": [
    "Replay Buffer\n",
    "- maxlen만큼 sample들을 저장하고 있으며, 그 이상으로 sample들이 들어오면 FIFO방식으로 eject한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "processed-freight",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, buffer_size, gamma):\n",
    "        # deque는 maxlen만큼만 FIFO\n",
    "        self._memory = deque(maxlen=buffer_size)\n",
    "        self.gamma = gamma\n",
    "    \n",
    "    def add(self, sample):\n",
    "        if sample['done']:\n",
    "            sample['predict'] = sample['reward']\n",
    "        else:\n",
    "            sample['predict'] = sample['reward'] + self.gamma*max(sample['qtarget'])\n",
    "        self._memory.append(sample)\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        # random하게 batch_size만큼 sample\n",
    "        mini_batch = random.sample(self._memory, batch_size)        \n",
    "        return mini_batch\n",
    "    \n",
    "    ###\n",
    "    def store(self, sample)\n",
    "        self._memory.append(sample)\n",
    "    \n",
    "    def sample_batch(self)\n",
    "        mini_batch = random.sample(self._memory, batch_size)        \n",
    "        return mini_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "swedish-disclaimer",
   "metadata": {},
   "source": [
    "## Playing Atari with Deep Reinforcement Learning, Algorithm 1\n",
    "- Q1. q 와 qtarget을 따로 둬야 하는 이유? - 행동하는 정책(q)과 학습하는 정책(q target)을 따로 둔다\n",
    "- Q2. 매 스텝마다 learn을 해야 하는것인가?\n",
    "- Q3. 언제 network(q, qtarget)의 parameter를 업데이트?\n",
    "- Q4. DQN의 action selection strategy? greedy? -- 그냥 epsilon greedy\n",
    "- Q5. zero_grad()?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "leading-healing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 2\n",
      "[-0.03020097  0.04437784  0.03425706  0.00074789]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'dim'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-9f16a861634a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mtotal_reward\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'state'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'reward'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'done'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'qtarget'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0mbuffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-25-016eed0b98c8>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/drl/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/drl/lib/python3.6/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/drl/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1686\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mTensor\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtens_ops\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mhas_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtens_ops\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1687\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtens_ops\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1688\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1689\u001b[0m         \u001b[0;31m# fused op is marginally faster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1690\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'dim'"
     ]
    }
   ],
   "source": [
    "# parameters\n",
    "episode_len = 1000\n",
    "plot_len = 50\n",
    "buffer_size = 1000\n",
    "batch_size = 10\n",
    "epsilon = 0.0\n",
    "gamma = 0.99\n",
    "\n",
    "# env\n",
    "env_list = ['CartPole-v0']\n",
    "\n",
    "# init class\n",
    "env = gym.make(env_list[0])\n",
    "agent = DQNAgent(env, epsilon)\n",
    "buffer = ReplayBuffer(buffer_size, gamma)\n",
    "\n",
    "# To-do\n",
    "# plot\n",
    "scores = []\n",
    "for i in range(episode_len):\n",
    "    state = env.reset()\n",
    "    total_reward = 0\n",
    "    avg_reward = 0\n",
    "    while True:\n",
    "        print(state)\n",
    "        action = agent.get_action(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "        sample = {'state':state, 'reward': reward, 'done': done, 'qtarget': agent.qtarget.forward(next_state)}\n",
    "        buffer.add(sample)\n",
    "        if done:\n",
    "            mini_batch = buffer.sample(batch_size)\n",
    "            agent.learn(mini_batch)\n",
    "            agent.update_qtarget_parameter()\n",
    "            total_reward = 0\n",
    "            break\n",
    "    avg_reward += total_reward\n",
    "    if i%plot_len == 0:\n",
    "        avg_reward = avg_reward / plot_len\n",
    "        scores.append(avg_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "solid-guatemala",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "episode_len = 1000\n",
    "plot_len = 50\n",
    "buffer_size = 1000\n",
    "batch_size = 10\n",
    "epsilon = 0.0\n",
    "gamma = 0.99\n",
    "\n",
    "# env\n",
    "env_list = ['CartPole-v0']\n",
    "\n",
    "# init class\n",
    "env = gym.make(env_list[0])\n",
    "agent = DQNAgent(env, epsilon)\n",
    "buffer = ReplayBuffer(buffer_size, gamma)\n",
    "\n",
    "# To-do\n",
    "# plot\n",
    "scores = []\n",
    "for i in range(episode_len):\n",
    "    score = 0\n",
    "    state = env.reset()\n",
    "    while True:\n",
    "        print(state)\n",
    "        action = agent.get_action(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        score += reward\n",
    "        sample = {'state': state, 'action': action, \n",
    "                  'next_state': next_state, 'reward': reward, 'done': done}\n",
    "        buffer.store(sample)\n",
    "        if done:\n",
    "            mini_batch = buffer.sample(batch_size)\n",
    "            agent.learn(mini_batch)\n",
    "            agent.update_qtarget_parameter()\n",
    "            total_reward = 0\n",
    "            break\n",
    "    \n",
    "    if (len(buffer._memory) >= batch_size) and (i%plot_len == 0):\n",
    "        scores.append(score/plot_len)\n",
    "        agent._plot(scores)\n",
    "        score = 0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
