{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "adjusted-significance",
   "metadata": {},
   "source": [
    "# DQN\n",
    "- NN과 같은 Nonlinear function approximator가 Q를 represent 할때 사용되는 경우 unstable하거나 diverge한다고 알려져왔다\n",
    "- 이런 불안정성은 몇가지 원인이 있다:\n",
    "    1) correlation present in the sequence of observations\n",
    "    2) correlation between Q and r+$\\gamma$ max Q\n",
    "- 따라서 저자는 두가지 Key Idea를 제시한다 : Replay Buffer, Fixed Q-target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "academic-consideration",
   "metadata": {},
   "source": [
    "# Jupyterlab Shortcuts\n",
    "- Shift + Enter : Run\n",
    "- Enter/Esc : Mode change(Edit/Command)\n",
    "- Fn + Up/Down : Move between cell \n",
    "- A / B : Insert Cell Above/Below\n",
    "- D, D : Delete selected cell"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "radical-works",
   "metadata": {},
   "source": [
    "# Random\n",
    "- random.random() : [0.0, 1.0) 사이의 실수를 반환\n",
    "- random.sample(population, k) : population sequence 에서 독립적으로 k length의 데이터를 sample\n",
    "- random.randrange(stop) : [0:stop] 사이의 정수 random하게 반환"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "foreign-hartford",
   "metadata": {},
   "source": [
    "# Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "electric-hepatitis",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "import numpy as np\n",
    "import random\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "balanced-emperor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 2\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "state = env.reset()\n",
    "\n",
    "# for discrete env\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.n\n",
    "\n",
    "print(state_dim, action_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prepared-louis",
   "metadata": {},
   "source": [
    "# Network\n",
    "- forward()에 들어갈 변수는 tensor, forward()의 return 값도 마찬가지로 tensor다\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "enabling-norway",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, hidden_dim=128):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(in_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, out_dim)\n",
    "    \n",
    "    def forward(self, state):\n",
    "        out = F.relu(self.fc1(state))\n",
    "        out = F.relu(self.fc2(out))\n",
    "        out = self.fc3(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "narrative-jones",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.0419, -0.4383,  0.1399,  0.0966])\n",
      "tensor([ 0.3319, -0.4922, -0.3422, -0.0949])\n",
      "tensor([-0.0419, -0.4383,  0.1399,  0.0966])\n",
      "tensor([-0.0419, -0.4383,  0.1399,  0.0966])\n"
     ]
    }
   ],
   "source": [
    "q = Net(state_dim, action_dim)\n",
    "qtarget = Net(state_dim, action_dim)\n",
    "\n",
    "# network의 weight값을 print\n",
    "weight_name = 'fc1.weight'\n",
    "print(q.state_dict()[weight_name][1])\n",
    "print(qtarget.state_dict()[weight_name][1])\n",
    "\n",
    "# network의 weight값을 다른 network의 weight값으로 맞춰줌\n",
    "qtarget.load_state_dict(q.state_dict())\n",
    "print(q.state_dict()[weight_name][1])\n",
    "print(qtarget.state_dict()[weight_name][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fossil-tiger",
   "metadata": {},
   "source": [
    "# Replay Buffer\n",
    "Replay Buffer는 다음의 두가지를 구현하여야 한다.\n",
    "1) 일정한 길이만큼 memory 유지, 일정 길이 이상으로 sample이 들어온다면 FIFO방식 으로 eject한다 => store()\n",
    "2) 특정한 길이(mini_batch)의 sample을 random하게 indexing => sample_batch()\n",
    "\n",
    "일반적으로, replay buffer를 구현할때 다음의 세가지 자료구조를 주로 이용한다\n",
    "1) collections.deque\n",
    "2) list\n",
    "3) numpy.ndarray\n",
    "\n",
    "deque은 maximum length를 설정해줄 수 있기 때문에 replaybuffer를 구현함에 있어서 매우 편하다. \n",
    "하지만, deque은 내부적으로는 doubly linked list이기 때문에 indexing operation이 시간이 걸린다는 단점이 있다.\n",
    "\n",
    "list의 경우, get item 의 amortized cost는 O(1)이기 때문에, 상대적으로 sample_batch()를 하는데 더 빠르다\n",
    "numpy.ndarray의 경우, homogeneous array of fixed-size items라는 사실 때문에 list보다도 더 빠르다(locality of reference의 이점을 얻을 수 있다)\n",
    "\n",
    "%(나머지)를 사용함으로써 max_size 만큼의 크기를 유지하며 FIFO가 되도록 구현하였다\n",
    "numpy.ndarray의 index를 numpy.ndarray로 주면 해당 인덱스의 값들만 ndarray 형태로 반환된다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "productive-lucas",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class ReplayBuffer:\n",
    "#     def __init__(self, buffer_size, batch_size):\n",
    "#         # deque는 maxlen만큼만 FIFO\n",
    "#         self._memory = deque(maxlen=buffer_size)\n",
    "#         self.batch_size = batch_size\n",
    "    \n",
    "#     def store(self, sample):\n",
    "#         # store 할때 target 값을 만들어 버리면 sample 됬을때의 q_target 를 이용하기 때문에 안된다\n",
    "#         # (s, a, s', r, d) 만 저장하여 나중에 loss를 계산 할때 target 값을 계산한다\n",
    "#         #sample = [sample['state'], sample['action'], sample['next_state'], \n",
    "#         #          sample['reward'], sample['done']]\n",
    "#         self._memory.append(sample)\n",
    "    \n",
    "#     def sample_batch(self):\n",
    "#         # _memory에는 list of list 가 저장된다\n",
    "#         # sample을 return할 때는 loss 계산의 편의를 위하여 dict of array를 내보낸다\n",
    "#         mini_batch = random.sample(self._memory, self.batch_size)\n",
    "#         # mini_batch = np.array(mini_batch)\n",
    "#         # return dict('state': mini_batch[:, 0], 'action': mini_batch[:, 1], \n",
    "#         #            'next_state': mini_batch[:, 2], 'reward': mini_batch[:, 3], \n",
    "#         #            'done': mini_batch[:, 4])\n",
    "#         return mini_batch\n",
    "    \n",
    "#     def __len__(self):\n",
    "#         return len(self._memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "negative-welsh",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, buffer_size, batch_size):\n",
    "        # deque는 maxlen만큼만 FIFO\n",
    "        self._memory = deque(maxlen=buffer_size)\n",
    "        self.batch_size = batch_size\n",
    "    \n",
    "    def store(self, sample):\n",
    "        # store 할때 target 값을 만들어 버리면 sample 됬을때의 q_target 를 이용하기 때문에 안된다\n",
    "        # (s, a, s', r, d) 만 저장하여 나중에 loss를 계산 할때 target 값을 계산한다\n",
    "        #sample = [sample['state'], sample['action'], sample['next_state'], \n",
    "        #          sample['reward'], sample['done']]\n",
    "        self._memory.append(sample)\n",
    "    \n",
    "    def sample_batch(self):\n",
    "        # _memory에는 list of list 가 저장된다\n",
    "        # sample을 return할 때는 loss 계산의 편의를 위하여 dict of array를 내보낸다\n",
    "        mini_batch = random.sample(self._memory, self.batch_size)\n",
    "        # mini_batch = np.array(mini_batch)\n",
    "        # return dict('state': mini_batch[:, 0], 'action': mini_batch[:, 1], \n",
    "        #            'next_state': mini_batch[:, 2], 'reward': mini_batch[:, 3], \n",
    "        #            'done': mini_batch[:, 4])\n",
    "        return mini_batch\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self._memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advised-split",
   "metadata": {},
   "source": [
    "# DQN\n",
    "- optimizer를 Net이 아닌 DQNAgent 클래스에서 선언하는 이유"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "colored-scope",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, env, epsilon, gamma):\n",
    "        # Env 가 Discrete일 경우\n",
    "        state_dim = env.observation_space.shape[0]\n",
    "        action_dim = env.action_space.n\n",
    "        \n",
    "        self.epsilon = epsilon\n",
    "        self.gamma = gamma\n",
    "        \n",
    "        # Q-learning은 off-policy algorithm\n",
    "        # 행동하는 정책과(q_behave) 학습에 이용하는 정책(q_target)을 따로 정의한다\n",
    "        self.q_behave = Net(state_dim, action_dim)\n",
    "        self.q_target = Net(state_dim, action_dim)\n",
    "        self._update_q_target_parameter()\n",
    "        \n",
    "        self.optimizer = optim.Adam(self.q_behave.parameters())\n",
    "    \n",
    "    def _update_q_target_parameter(self):\n",
    "        # q_target의 parameter를 q_behave의 parameter값으로 update\n",
    "        self.q_target.load_state_dict(self.q_behave.state_dict())    \n",
    "    \n",
    "    def get_action(self, state):\n",
    "        # 일반적인 epsilon-greedy 를 이용한다\n",
    "        # detach()는 왜 해주지??\n",
    "        if self.epsilon < random.random():\n",
    "            selected_action = random.randrange(action_dim)\n",
    "        else:\n",
    "            state = torch.FloatTensor(state)\n",
    "            selected_action = self.q_behave.forward(state).argmax()\n",
    "            selected_action = selected_action.detach().numpy()\n",
    "        return selected_action\n",
    "    \n",
    "    def update_model(self, samples):\n",
    "        # zero_grad()는 왜 해주지??\n",
    "        loss = self._compute_dqn_loss(samples)\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        return loss.item()\n",
    "    \n",
    "    def _compute_dqn_loss(self, samples):\n",
    "        # 왜 l1 loss 사용?\n",
    "        curr_q_values = []\n",
    "        targets = []\n",
    "        for sample in samples:\n",
    "            state = torch.FloatTensor(sample['state'])\n",
    "            curr_q_value = self.q_behave.forward(state)\n",
    "            curr_q_value = curr_q_value[sample['action']].detach().numpy()\n",
    "            if sample['done']:\n",
    "                target = sample['reward']\n",
    "            else:\n",
    "                max_q_target = self.q_target.forward(state).max()\n",
    "                max_q_target = max_q_target.detach().numpy()\n",
    "                target = sample['reward'] + self.gamma * max_q_target\n",
    "            curr_q_values.append(curr_q_value)\n",
    "            targets.append(target)\n",
    "        \n",
    "        curr_q_values = torch.tensor(np.array(curr_q_values))\n",
    "        targets = torch.tensor(np.array(targets))\n",
    "        loss = F.smooth_l1_loss(curr_q_values, targets)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def _plot(self, scores):\n",
    "        plt.plot(scores)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "parallel-palmer",
   "metadata": {},
   "source": [
    "## Playing Atari with Deep Reinforcement Learning, Algorithm 1\n",
    "- Q1. q 와 qtarget을 따로 둬야 하는 이유? - 행동하는 정책(q)과 학습하는 정책(q target)을 따로 둔다\n",
    "- Q2. 매 스텝마다 learn을 해야 하는것인가?\n",
    "- Q3. 언제 network(q, qtarget)의 parameter를 업데이트?\n",
    "- Q4. DQN의 action selection strategy? greedy? -- 그냥 epsilon greedy\n",
    "- Q5. zero_grad()?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "social-paraguay",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 1 required positional argument: 'gamma'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-c14baabfa3ca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# init class\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0magent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDQNAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mReplayBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() missing 1 required positional argument: 'gamma'"
     ]
    }
   ],
   "source": [
    "# parameters\n",
    "episode_len = 1000\n",
    "plot_len = 50\n",
    "buffer_size = 1000\n",
    "batch_size = 10\n",
    "epsilon = 0.0\n",
    "gamma = 0.99\n",
    "\n",
    "# env\n",
    "env_list = ['CartPole-v0']\n",
    "\n",
    "# init class\n",
    "env = gym.make(env_list[0])\n",
    "agent = DQNAgent(env, epsilon)\n",
    "buffer = ReplayBuffer(buffer_size, gamma)\n",
    "\n",
    "# To-do\n",
    "# plot\n",
    "scores = []\n",
    "for i in range(episode_len):\n",
    "    state = env.reset()\n",
    "    total_reward = 0\n",
    "    avg_reward = 0\n",
    "    while True:\n",
    "        print(state)\n",
    "        action = agent.get_action(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "        sample = {'state':state, 'reward': reward, 'done': done, 'qtarget': agent.qtarget.forward(next_state)}\n",
    "        buffer.add(sample)\n",
    "        if done:\n",
    "            mini_batch = buffer.sample(batch_size)\n",
    "            agent.learn(mini_batch)\n",
    "            agent.update_qtarget_parameter()\n",
    "            total_reward = 0\n",
    "            break\n",
    "    avg_reward += total_reward\n",
    "    if i%plot_len == 0:\n",
    "        avg_reward = avg_reward / plot_len\n",
    "        scores.append(avg_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "historic-barbados",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "[{'state': array([-0.03751107, -0.02465864, -0.02697723,  0.00018051]), 'action': array(1), 'next_state': array([-8.70587177e-04,  9.53482391e-01, -8.73542159e-02, -1.52311424e+00]), 'reward': 1.0, 'done': False}, {'state': array([-0.03751107, -0.02465864, -0.02697723,  0.00018051]), 'action': array(1), 'next_state': array([ 0.01819906,  1.14954412, -0.1178165 , -1.84173496]), 'reward': 1.0, 'done': False}, {'state': array([-0.03751107, -0.02465864, -0.02697723,  0.00018051]), 'action': array(1), 'next_state': array([ 0.04118994,  1.34575279, -0.1546512 , -2.16856567]), 'reward': 1.0, 'done': False}, {'state': array([-0.03751107, -0.02465864, -0.02697723,  0.00018051]), 'action': array(1), 'next_state': array([-0.02726074,  0.56190294, -0.04503057, -0.90484622]), 'reward': 1.0, 'done': False}, {'state': array([-0.04980659, -0.01090533,  0.01153459,  0.02333287]), 'action': array(1), 'next_state': array([-0.0500247 ,  0.18404932,  0.01200124, -0.26568856]), 'reward': 1.0, 'done': False}, {'state': array([-0.03751107, -0.02465864, -0.02697723,  0.00018051]), 'action': array(1), 'next_state': array([-0.03800424,  0.1708396 , -0.02697362, -0.30089051]), 'reward': 1.0, 'done': False}, {'state': array([-0.03751107, -0.02465864, -0.02697723,  0.00018051]), 'action': array(1), 'next_state': array([ 0.068105  ,  1.54201004, -0.19802251, -2.50472456]), 'reward': 1.0, 'done': False}, {'state': array([-0.03751107, -0.02465864, -0.02697723,  0.00018051]), 'action': array(1), 'next_state': array([-0.01602268,  0.75760485, -0.0631275 , -1.21133602]), 'reward': 1.0, 'done': False}, {'state': array([-0.03751107, -0.02465864, -0.02697723,  0.00018051]), 'action': array(1), 'next_state': array([ 0.0989452 ,  1.73813872, -0.248117  , -2.8510072 ]), 'reward': 1.0, 'done': True}, {'state': array([-0.03751107, -0.02465864, -0.02697723,  0.00018051]), 'action': array(1), 'next_state': array([-0.03458745,  0.36633542, -0.03299143, -0.60195685]), 'reward': 1.0, 'done': False}]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-e460df745b0e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0mmini_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmini_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmini_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m             \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0mupdate_cnt\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-22-485162d9ec8e>\u001b[0m in \u001b[0;36mupdate_model\u001b[0;34m(self, samples)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ddrl/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ddrl/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    130\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "# rainbow-is-all-you-need 참고하여 수정한 버전\n",
    "\n",
    "# parameters\n",
    "episode_len = 1000\n",
    "plot_len = 50\n",
    "buffer_size = 1000\n",
    "batch_size = 10\n",
    "epsilon = 0.8\n",
    "gamma = 0.99\n",
    "\n",
    "target_update = 10\n",
    "\n",
    "# env\n",
    "env_list = ['CartPole-v0']\n",
    "\n",
    "# init class\n",
    "env = gym.make(env_list[0])\n",
    "agent = DQNAgent(env, epsilon, gamma)\n",
    "buffer = ReplayBuffer(buffer_size, batch_size)\n",
    "\n",
    "# To-do\n",
    "# plot\n",
    "score = 0\n",
    "scores = []\n",
    "losses = []\n",
    "for i in range(episode_len):\n",
    "    state = env.reset()\n",
    "    while True:\n",
    "        action = agent.get_action(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        score += reward\n",
    "        sample = {'state': state, 'action': action, \n",
    "                  'next_state': next_state, 'reward': reward, 'done': done}\n",
    "        buffer.store(sample)\n",
    "        print(len(buffer))\n",
    "        \n",
    "        if done:\n",
    "            scores.append(score)\n",
    "            score = 0\n",
    "            break\n",
    "        \n",
    "        if (len(buffer) >= batch_size):\n",
    "            mini_batch = buffer.sample_batch()\n",
    "            print(mini_batch)\n",
    "            loss = agent.update_model(mini_batch)\n",
    "            losses.append(loss)\n",
    "            update_cnt += 1\n",
    "            \n",
    "            if update_cnt % target_update == 0:\n",
    "                agent.update_qtarget_parameter()    \n",
    "            \n",
    "    if i%plot_len == 0:\n",
    "#         agent._plot(plot_len, scores, losses)\n",
    "        agent._plot(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "complimentary-defeat",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
