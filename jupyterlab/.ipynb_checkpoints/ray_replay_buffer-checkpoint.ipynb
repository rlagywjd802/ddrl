{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "selected-insertion",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Framework\n",
    "Buffer\n",
    "Actor : 각 Env에서 경험한 것을 buffer에 넘겨준다\n",
    "Learner : 공유 buffer를 통해 학습을 진행"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "meaning-cabin",
   "metadata": {},
   "source": [
    "# Questions\n",
    "1. Env도 Agent 개수만큼 만들어야 하는가?\n",
    "2. 공유 메모리 생성 방법?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "breathing-intranet",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "import time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "powered-rings",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-06 08:32:06,793\tINFO services.py:1173 -- View the Ray dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265\u001b[39m\u001b[22m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'node_ip_address': '192.168.0.2',\n",
       " 'raylet_ip_address': '192.168.0.2',\n",
       " 'redis_address': '192.168.0.2:6379',\n",
       " 'object_store_address': '/tmp/ray/session_2021-02-06_08-32-06_319651_23789/sockets/plasma_store',\n",
       " 'raylet_socket_name': '/tmp/ray/session_2021-02-06_08-32-06_319651_23789/sockets/raylet',\n",
       " 'webui_url': '127.0.0.1:8265',\n",
       " 'session_dir': '/tmp/ray/session_2021-02-06_08-32-06_319651_23789',\n",
       " 'metrics_export_port': 52203,\n",
       " 'node_id': '67557e630771e2f94d812582443967d98fd5aa56'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ray.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "figured-garden",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 간단한 env를 정의하겠습니다. environment의 일반적인 메소드만 넣고 어떤 의미가 있는 행동이나 상태를 정의한 것은 아닙니다.\n",
    "class Env:        \n",
    "    def reset(self):\n",
    "        return np.ones((2,2))\n",
    "\n",
    "    def step(self, action):\n",
    "        # state, reward, done 모두 random하게 지정. state의 크기는 2x2 차원을 가지는 2차원 matrix.\n",
    "        state = action*np.random.randn(2, 2)\n",
    "        reward = np.sum(state)\n",
    "\n",
    "        # done은 numpy의 random.randn 이 0.06 보다 작을 때만 1을 주었습니다. 더 자주 done이 발생하도록 하고 싶다면, 0.06을 더 키우면 됩니다.\n",
    "        done = 1 if abs(np.random.randn())<0.06 else 0\n",
    "        return state, reward, done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "veterinary-gothic",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 Agent에서 생성된 sample을 공유메모리인 buffer에 저장\n",
    "# 왜 buffer가 ray함수여야 하는가?\n",
    "# --> 각 쓰레드에서 얻은 sample를 저장할때 buffer 함수 호출을 하기 때문에\n",
    "# --> 그렇담, store()만 ray함수로 만들면 되지 않는가?\n",
    "@ray.remote\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, buffer_size=1000):\n",
    "        self._memory = deque(max_len=buffer_size)\n",
    "        \n",
    "    def store(self, sample):\n",
    "        self._memory.append(sample)\n",
    "        \n",
    "    def sample_batch(self):\n",
    "        pass    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "educational-insertion",
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "class Agent:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def get_action(self, state):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "precise-thumb",
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_len = 100\n",
    "num_workers = 4\n",
    "\n",
    "@ray.remote\n",
    "def train(env, agent, episode_len):\n",
    "    for i in range(episode_len):\n",
    "        state = env.reset()\n",
    "        while True:\n",
    "            action = agent.get_action(state)\n",
    "            next_state, reward, done = env.step(action)\n",
    "            buffer.store(state, action, reward, next_state, done)\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "env = Env()\n",
    "agents = [Agent.remote() for _ in range(num_workers)]\n",
    "\n",
    "for i in range(len(agents)):\n",
    "    train.remote(env, agents[i], episode_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "renewable-castle",
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "class Actor:\n",
    "    def __init__(self, buffer, idx, episode_len=100):\n",
    "        # 공유 메모리 생성을 위해서, 외부에서 메모리를 불러온다\n",
    "        # 그 후, remote를 이용하여 외부 메모리의 함수를 실행한다\n",
    "        # env는 agent마다 독립적으로 가져야 하기 때문에 내부에서 클래스 선언한다\n",
    "        self.env = Env()\n",
    "        self.buffer = buffer\n",
    "        \n",
    "        self.episode_len = episode_len\n",
    "    \n",
    "    def explore(self):\n",
    "        for i in range(self.episode_len):\n",
    "            state = self.env.reset()\n",
    "            while True:\n",
    "                action = self.np.random.randint(3)\n",
    "                next_state, reward, done = self.env.step(action)\n",
    "                sample = {'state': state, 'action': action, \n",
    "                          'reward': reward, 'next_state': next_state, 'done': done}            \n",
    "                self.buffer.store.remote(sample)\n",
    "                if done:\n",
    "                    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "animal-organization",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 원래는 Agent(Actor)에서 Q Network를 선언했다\n",
    "# 하지만 multi-agent의 경우, Agent의 안에서 Q Network를 생성하면 여러개가 된다\n",
    "# (q_behave와 q_target은 하나여야 함)\n",
    "# 따라서 Network의 값을 가지고 있는 별도의 Class Learner를 생성한다 \n",
    "class Learner:\n",
    "    def __init__(self):\n",
    "        self.q_behave = Net()\n",
    "        self.q_target = Net()\n",
    "    def update_model(self):\n",
    "        pass\n",
    "    def update_q_target_parameter(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "micro-nelson",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_actors = 10\n",
    "batch_size = 20\n",
    "\n",
    "buffer = ReplayBuffer(batch_size)\n",
    "learner = Learner()\n",
    "\n",
    "for idx in range(num_actors):\n",
    "    actor = Actor.remote(buffer, idx) \n",
    "    actor.explore.remote()\n",
    "\n",
    "learner.update_model()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
